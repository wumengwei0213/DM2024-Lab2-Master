{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom datasets import Dataset, DatasetDict\n\nimport json\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport panel as pn\nimport warnings; warnings.filterwarnings('ignore')\nimport plotly.express as px\n\nfrom transformers import Trainer, TrainingArguments\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:19:38.078509Z","iopub.execute_input":"2024-12-04T14:19:38.079331Z","iopub.status.idle":"2024-12-04T14:19:46.397905Z","shell.execute_reply.started":"2024-12-04T14:19:38.079295Z","shell.execute_reply":"2024-12-04T14:19:46.396936Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Step 1: Load and preprocess the dataset\n\n# Read data\ndata = []\nwith open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\n \nf.close()\nemotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\ndata_id = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n\n\n# Create DataFrame from JSON\ndf = pd.DataFrame(data)\n_source = df['_source'].apply(lambda x: x['tweet'])\ndf = pd.DataFrame({\n    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n    'text': _source.apply(lambda x: x['text']),\n})\n\n# Merge emotion and data_identification\ndf = df.merge(emotion, on='tweet_id', how='left')  # Add emotion column\ndf = df.merge(data_id, on='tweet_id', how='left')  # Add identification column\n\n# Display resulting DataFrame\n# Check if all columns are included: tweet_id, text, emotion, identification\nprint(df.head())  \n\n# Split into train_data and test_data\ntrain_data = df[df['identification'] == 'train']\ntest_data = df[df['identification'] == 'test']\n\n# Verify splits\nprint(f\"Train data: {len(train_data)} rows\")\nprint(f\"Test data: {len(test_data)} rows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:19:46.399317Z","iopub.execute_input":"2024-12-04T14:19:46.399738Z","iopub.status.idle":"2024-12-04T14:20:25.190712Z","shell.execute_reply.started":"2024-12-04T14:19:46.399710Z","shell.execute_reply":"2024-12-04T14:20:25.189755Z"}},"outputs":[{"name":"stdout","text":"   tweet_id                                               text       emotion  \\\n0  0x376b20  People who post \"add me on #Snapchat\" must be ...  anticipation   \n1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...       sadness   \n2  0x28b412  Confident of your obedience, I write to you, k...           NaN   \n3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>          fear   \n4  0x2de201  \"Trust is not the same as faith. A friend is s...           NaN   \n\n  identification  \n0          train  \n1          train  \n2           test  \n3          train  \n4           test  \nTrain data: 1455563 rows\nTest data: 411972 rows\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n# Encode labels\nclass_names = ['anger', 'anticipation', 'disgust', 'fear', 'sadness', 'surprise', 'trust', 'joy']\ntrain_data['label'] = train_data['emotion'].astype(\"category\").cat.codes\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:24:48.563313Z","iopub.execute_input":"2024-12-04T14:24:48.564155Z","iopub.status.idle":"2024-12-04T14:24:48.647714Z","shell.execute_reply.started":"2024-12-04T14:24:48.564123Z","shell.execute_reply":"2024-12-04T14:24:48.646782Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Reset index before conversion to remove the index column\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\n# Convert to Hugging Face DatasetDict\nemotions = DatasetDict({\n    \"train\": Dataset.from_pandas(train_data[['text', 'label']]),\n    \"test\": Dataset.from_pandas(test_data[['text']])\n})\n\nemotions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:26:39.724040Z","iopub.execute_input":"2024-12-04T14:26:39.724439Z","iopub.status.idle":"2024-12-04T14:26:41.965135Z","shell.execute_reply.started":"2024-12-04T14:26:39.724409Z","shell.execute_reply":"2024-12-04T14:26:41.964190Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1455563\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 411972\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Step 2: Tokenize the datasets\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True, max_length=32)\n\nemotions = emotions.map(tokenize, batched=True)\nemotions = emotions.rename_column(\"label\", \"labels\")\nemotions.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:26:59.933373Z","iopub.execute_input":"2024-12-04T14:26:59.933726Z","iopub.status.idle":"2024-12-04T14:28:40.353238Z","shell.execute_reply.started":"2024-12-04T14:26:59.933696Z","shell.execute_reply":"2024-12-04T14:28:40.351176Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1455563 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"837b61a3a07d447083d61de48a35e7c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/411972 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708cc143f12648ea8c84ae4884a2ce82"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      8\u001b[0m emotions \u001b[38;5;241m=\u001b[39m emotions\u001b[38;5;241m.\u001b[39mmap(tokenize, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m emotions \u001b[38;5;241m=\u001b[39m \u001b[43memotions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m emotions\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:405\u001b[0m, in \u001b[0;36mDatasetDict.rename_column\u001b[0;34m(self, original_column_name, new_column_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mrename_column(original_column_name\u001b[38;5;241m=\u001b[39moriginal_column_name, new_column_name\u001b[38;5;241m=\u001b[39mnew_column_name)\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:406\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[0;32m--> 406\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_column_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2193\u001b[0m, in \u001b[0;36mDataset.rename_column\u001b[0;34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[0m\n\u001b[1;32m   2191\u001b[0m dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[0;32m-> 2193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2196\u001b[0m     )\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Original column name label not in the dataset. Current columns in the dataset: ['text', 'input_ids', 'attention_mask']"],"ename":"ValueError","evalue":"Original column name label not in the dataset. Current columns in the dataset: ['text', 'input_ids', 'attention_mask']","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# Step 3: Load the model\nnum_labels = len(class_names)  # Number of classes updated to 8\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:22:37.325797Z","iopub.status.idle":"2024-12-04T14:22:37.326077Z","shell.execute_reply.started":"2024-12-04T14:22:37.325940Z","shell.execute_reply":"2024-12-04T14:22:37.325954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Define performance metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:22:37.327150Z","iopub.status.idle":"2024-12-04T14:22:37.327612Z","shell.execute_reply.started":"2024-12-04T14:22:37.327376Z","shell.execute_reply":"2024-12-04T14:22:37.327399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    log_level=\"error\",\n    report_to=\"none\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:22:37.329093Z","iopub.status.idle":"2024-12-04T14:22:37.329454Z","shell.execute_reply.started":"2024-12-04T14:22:37.329261Z","shell.execute_reply":"2024-12-04T14:22:37.329276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Fine-tune the model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=emotions[\"train\"],\n    tokenizer=tokenizer\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:22:37.330383Z","iopub.status.idle":"2024-12-04T14:22:37.330663Z","shell.execute_reply.started":"2024-12-04T14:22:37.330523Z","shell.execute_reply":"2024-12-04T14:22:37.330542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Predict on the test set\nemotions[\"test\"] = emotions[\"test\"].map(tokenize, batched=True)\npredictions = trainer.predict(emotions[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:22:37.332099Z","iopub.status.idle":"2024-12-04T14:22:37.332441Z","shell.execute_reply.started":"2024-12-04T14:22:37.332255Z","shell.execute_reply":"2024-12-04T14:22:37.332271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Prepare submission\ntest_data[\"emotion\"] = [class_names[pred] for pred in predictions.predictions.argmax(-1)]\ntest_data[[\"id\", \"emotion\"]].to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created: submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:22:37.333770Z","iopub.status.idle":"2024-12-04T14:22:37.334067Z","shell.execute_reply.started":"2024-12-04T14:22:37.333926Z","shell.execute_reply":"2024-12-04T14:22:37.333940Z"}},"outputs":[],"execution_count":null}]}